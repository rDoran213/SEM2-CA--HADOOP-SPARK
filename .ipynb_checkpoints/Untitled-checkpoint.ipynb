{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dc72ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44092e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secret_data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa7886b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_start = \"2020-01-01\"\n",
    "\n",
    "api_key_arg = \"&api_key=\" + secret_data.get(\"api_key\")\n",
    "api_start_arg = \"&start=\" + api_data_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9917eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###data_endpoint = \"https://api.eia.gov/v2/electricity/rto/daily-region-sub-ba-data/data/?frequency=daily&data[0]=value&start=2025-03-15&sort[0][column]=period&sort[0][direction]=desc&offset=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "796e3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_endpoint = \"https://api.eia.gov/v2/electricity/rto/daily-region-data/data/?frequency=daily&data[0]=value&facets[respondent][]=NY&facets[timezone][]=Eastern&facets[type][]=D&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6532a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecd595db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request succeeded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.get(data_endpoint+api_start_arg+api_key_arg)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"request succeeded\")\n",
    "\n",
    "else:\n",
    "    print(f\"request failed, code:  {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2f980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf4188b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(data.get(\"response\").get(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c0aac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_csv('raw-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf139802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:==========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------------+----+---------+--------+--------------------+------+-------------+\n",
      "|_c0|    period|respondent|respondent-name|type|type-name|timezone|timezone-description| value|  value-units|\n",
      "+---+----------+----------+---------------+----+---------+--------+--------------------+------+-------------+\n",
      "|  0|2025-03-30|        NY|       New York|   D|   Demand| Eastern|             Eastern|360251|megawatthours|\n",
      "|  1|2025-03-29|        NY|       New York|   D|   Demand| Eastern|             Eastern|356584|megawatthours|\n",
      "|  2|2025-03-28|        NY|       New York|   D|   Demand| Eastern|             Eastern|373815|megawatthours|\n",
      "|  3|2025-03-27|        NY|       New York|   D|   Demand| Eastern|             Eastern|371568|megawatthours|\n",
      "|  4|2025-03-26|        NY|       New York|   D|   Demand| Eastern|             Eastern|383131|megawatthours|\n",
      "|  5|2025-03-25|        NY|       New York|   D|   Demand| Eastern|             Eastern|378674|megawatthours|\n",
      "|  6|2025-03-24|        NY|       New York|   D|   Demand| Eastern|             Eastern|398827|megawatthours|\n",
      "|  7|2025-03-23|        NY|       New York|   D|   Demand| Eastern|             Eastern|351612|megawatthours|\n",
      "|  8|2025-03-22|        NY|       New York|   D|   Demand| Eastern|             Eastern|354942|megawatthours|\n",
      "|  9|2025-03-21|        NY|       New York|   D|   Demand| Eastern|             Eastern|360106|megawatthours|\n",
      "| 10|2025-03-20|        NY|       New York|   D|   Demand| Eastern|             Eastern|365904|megawatthours|\n",
      "| 11|2025-03-19|        NY|       New York|   D|   Demand| Eastern|             Eastern|354325|megawatthours|\n",
      "| 12|2025-03-18|        NY|       New York|   D|   Demand| Eastern|             Eastern|367109|megawatthours|\n",
      "| 13|2025-03-17|        NY|       New York|   D|   Demand| Eastern|             Eastern|383316|megawatthours|\n",
      "| 14|2025-03-16|        NY|       New York|   D|   Demand| Eastern|             Eastern|350585|megawatthours|\n",
      "| 15|2025-03-15|        NY|       New York|   D|   Demand| Eastern|             Eastern|353508|megawatthours|\n",
      "| 16|2025-03-14|        NY|       New York|   D|   Demand| Eastern|             Eastern|362794|megawatthours|\n",
      "| 17|2025-03-13|        NY|       New York|   D|   Demand| Eastern|             Eastern|374367|megawatthours|\n",
      "| 18|2025-03-12|        NY|       New York|   D|   Demand| Eastern|             Eastern|380035|megawatthours|\n",
      "| 19|2025-03-11|        NY|       New York|   D|   Demand| Eastern|             Eastern|360839|megawatthours|\n",
      "+---+----------+----------+---------------+----+---------+--------+--------------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/31 22:13:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , period, respondent, respondent-name, type, type-name, timezone, timezone-description, value, value-units\n",
      " Schema: _c0, period, respondent, respondent-name, type, type-name, timezone, timezone-description, value, value-units\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/staging/raw-data.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.system(\"hdfs dfs -put -f ./raw-data.csv /staging/\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Power Demand Tracker\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv('hdfs:///staging/raw-data.csv', header=True, inferSchema=True)\n",
    "\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac1186ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b191230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filtered_spark_df = df_spark.select(\n",
    "    col(\"period\").alias(\"Date\"),\n",
    "    col(\"value\").alias(\"MWh\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb85e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08da28e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      Date|   MWh|\n",
      "+----------+------+\n",
      "|2025-03-30|360251|\n",
      "|2025-03-29|356584|\n",
      "|2025-03-28|373815|\n",
      "|2025-03-27|371568|\n",
      "|2025-03-26|383131|\n",
      "|2025-03-25|378674|\n",
      "|2025-03-24|398827|\n",
      "|2025-03-23|351612|\n",
      "|2025-03-22|354942|\n",
      "|2025-03-21|360106|\n",
      "|2025-03-20|365904|\n",
      "|2025-03-19|354325|\n",
      "|2025-03-18|367109|\n",
      "|2025-03-17|383316|\n",
      "|2025-03-16|350585|\n",
      "|2025-03-15|353508|\n",
      "|2025-03-14|362794|\n",
      "|2025-03-13|374367|\n",
      "|2025-03-12|380035|\n",
      "|2025-03-11|360839|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96237ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = raw_df[['period', 'value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c3ceb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995e6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40103052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----+\n",
      "|      Date|   MWh|Day|Month|\n",
      "+----------+------+---+-----+\n",
      "|2025-03-30|360251|  1|    3|\n",
      "|2025-03-29|356584|  7|    3|\n",
      "|2025-03-28|373815|  6|    3|\n",
      "|2025-03-27|371568|  5|    3|\n",
      "|2025-03-26|383131|  4|    3|\n",
      "|2025-03-25|378674|  3|    3|\n",
      "|2025-03-24|398827|  2|    3|\n",
      "|2025-03-23|351612|  1|    3|\n",
      "|2025-03-22|354942|  7|    3|\n",
      "|2025-03-21|360106|  6|    3|\n",
      "|2025-03-20|365904|  5|    3|\n",
      "|2025-03-19|354325|  4|    3|\n",
      "|2025-03-18|367109|  3|    3|\n",
      "|2025-03-17|383316|  2|    3|\n",
      "|2025-03-16|350585|  1|    3|\n",
      "|2025-03-15|353508|  7|    3|\n",
      "|2025-03-14|362794|  6|    3|\n",
      "|2025-03-13|374367|  5|    3|\n",
      "|2025-03-12|380035|  4|    3|\n",
      "|2025-03-11|360839|  3|    3|\n",
      "+----------+------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df = filtered_spark_df.withColumn(\"Day\", F.dayofweek(\"Date\"))\n",
    "features_df = features_df.withColumn(\"Month\", F.month(\"Date\"))\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2af5aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#features_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"hdfs:///unscaled/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4811196d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2b13dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518109ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4f32622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark_test = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"hdfs:///unscaled/\")\n",
    "#df_spark_test.show()\n",
    "\n",
    "\n",
    "##hdfs dfs -getmerge /unscaled/ ./unscaled-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad4dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e04593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN_df = features_df.select(\"Date\", \"MWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52b8daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def scale_features(df):\n",
    "    this_df = df\n",
    "    \n",
    "    for index, column in enumerate(df.columns):\n",
    "        \n",
    "        if column != \"Date\":\n",
    "        \n",
    "            assembler = VectorAssembler(inputCols=[column], outputCol=\"feature_\"+column)\n",
    "\n",
    "            df_vec = assembler.transform(this_df)\n",
    "            \n",
    "            print(df_vec)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler(inputCol=\"feature_\"+column, outputCol=\"scaled_\"+column)\n",
    "            scaler_model = scaler.fit(df_vec)\n",
    "            scaled_df = scaler_model.transform(df_vec)\n",
    "\n",
    "\n",
    "            extract_element = udf(lambda v: float(v[0]), DoubleType())\n",
    "            \n",
    "            print(\"scaled_\"+column)\n",
    "\n",
    "            this_df = scaled_df.withColumn(\"scaled_\"+column, extract_element(col(\"scaled_\"+column)))\n",
    "\n",
    "    return this_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c71a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Date: date, MWh: int, Day: int, Month: int, feature_MWh: vector]\n",
      "scaled_MWh\n",
      "DataFrame[Date: date, MWh: int, Day: int, Month: int, feature_MWh: vector, scaled_MWh: double, feature_Day: vector]\n",
      "scaled_Day\n",
      "DataFrame[Date: date, MWh: int, Day: int, Month: int, feature_MWh: vector, scaled_MWh: double, feature_Day: vector, scaled_Day: double, feature_Month: vector]\n",
      "scaled_Month\n"
     ]
    }
   ],
   "source": [
    "scaled_features_df = scale_features(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12c5877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_scaled_feature = scaled_features_df.select(\"Date\", \"scaled_MWh\", \"scaled_Day\", \"scaled_Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2917bbc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+\n",
      "|      Date|         scaled_MWh|         scaled_Day|       scaled_Month|\n",
      "+----------+-------------------+-------------------+-------------------+\n",
      "|2025-03-30|0.16638138061417404|                0.0|0.18181818181818182|\n",
      "|2025-03-29|0.15467081395942978|                1.0|0.18181818181818182|\n",
      "|2025-03-28|0.20969802258443618| 0.8333333333333333|0.18181818181818182|\n",
      "|2025-03-27|0.20252222676408974| 0.6666666666666666|0.18181818181818182|\n",
      "|2025-03-26|0.23944867405855605|                0.5|0.18181818181818182|\n",
      "|2025-03-25|0.22521524193960452| 0.3333333333333333|0.18181818181818182|\n",
      "|2025-03-24|0.28957385928159013|0.16666666666666666|0.18181818181818182|\n",
      "|2025-03-23|0.13879272903786216|                0.0|0.18181818181818182|\n",
      "|2025-03-22|0.14942708599458382|                1.0|0.18181818181818182|\n",
      "|2025-03-21|0.16591832302897144| 0.8333333333333333|0.18181818181818182|\n",
      "|2025-03-20|0.18443423943590007| 0.6666666666666666|0.18181818181818182|\n",
      "|2025-03-19| 0.1474566961320321|                0.5|0.18181818181818182|\n",
      "|2025-03-18|  0.188282407643963| 0.3333333333333333|0.18181818181818182|\n",
      "|2025-03-17| 0.2400394716672628|0.16666666666666666|0.18181818181818182|\n",
      "|2025-03-16|0.13551300393439272|                0.0|0.18181818181818182|\n",
      "|2025-03-15|0.14484760615195955|                1.0|0.18181818181818182|\n",
      "|2025-03-14| 0.1745024526084513| 0.8333333333333333|0.18181818181818182|\n",
      "|2025-03-13|0.21146083490879364| 0.6666666666666666|0.18181818181818182|\n",
      "|2025-03-12|0.22956159623933373|                0.5|0.18181818181818182|\n",
      "|2025-03-11|0.16825915895968527| 0.3333333333333333|0.18181818181818182|\n",
      "+----------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_scaled_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d10b78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_scaled_feature.write.mode(\"overwrite\").option(\"header\", True).csv(\"hdfs:///transformed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa924be",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## hdfs dfs -getmerge /transformed/ ./transformed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98421c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_test = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"hdfs:///transformed/\")\n",
    "df_spark_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cabf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193101eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c522bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bed8d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#RNN_df_scaled.write.mode(\"overwrite\").option(\"header\", True).csv(\"hdfs:///transformed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## hdfs dfs -getmerge /transformed/ ./transformed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28057508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505dd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402016f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e187a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf477623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee8beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0263752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0070df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767e8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6a54ea",
   "metadata": {},
   "source": [
    "To install keras and tensorflow\n",
    "\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install python3-venv python3-pip -y\n",
    "\n",
    "python3 -m venv keras-env\n",
    "source keras-env/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install keras tensorflow\n",
    "\n",
    "\n",
    "source keras-env/bin/activate\n",
    "source /home/hduser/Documents/GitHub/CCT-SEM2-CA1/SEM2-CA--HADOOP-SPARK/keras-env/bin/activate\n",
    "\n",
    "\n",
    "\n",
    "pip install jupyter ipykernel\n",
    "\n",
    "python -m ipykernel install --user --name=keras-env --display-name=\"Python (keras-env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3aa09-3ae5-4884-86e4-3f793f2d0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c30150-c6fd-4c59-b3bb-a86332ccc391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cb822-4c7e-4854-b11d-2c217af7aa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fcf08-76ac-4868-b214-54426d09e46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300853a6-7cbb-4dee-b349-ddb31440a617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9ed2b-5cac-49da-ba49-19b85ac35d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da0f1d-fa2d-4852-b281-3becb0335fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfe179-25f6-4355-baf0-6174e0700e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc7552-c982-4c01-9ffe-0e63af8087ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ceae2b-2f96-44d0-b9eb-d56ccbd54f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459f69c-46ea-4a56-b3fe-243c0327571a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f600a4-b503-4d00-b578-a1fc25b119e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e0ebc-c2b6-4d84-b61b-82dfbe65c8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395ac39-bdc1-4545-b41e-43eff4046160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##pip install keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras / Deep Learning\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "#from keras import optimizers, regularizers\n",
    "#from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elephas for Deep Learning on Spark\n",
    "#from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa668f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "#df['period'] = pd.to_datetime(df['period'])\n",
    "#df.set_index('period', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef875c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#df.plot(ax=ax)\n",
    "\n",
    "#ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7467312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca302ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa096091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b896346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = test_df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = scaler.fit_transform(training_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler.fit(df)\n",
    "#scaled_data = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e537ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63103ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_test_df = pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc99cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904caad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ca69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12866841",
   "metadata": {},
   "source": [
    "Save Data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save df as csv\n",
    "\n",
    "##df.to_csv('test-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload to Hadoop\n",
    "\n",
    "#import os\n",
    "\n",
    "#os.system(\"hdfs dfs -put -f ./test-data.csv /testdata/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6743bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#import os\n",
    "\n",
    "#os.system(\"hdfs dfs -put -f ./test-data.csv /testdata/\")\n",
    "\n",
    "\n",
    "#park = SparkSession.builder \\\n",
    "   # .appName(\"Power Demand Tracker\") \\\n",
    " #  .getOrCreate()\n",
    "\n",
    "#df_spark = spark.read.csv('hdfs:///testdata/test-data.csv', header=True, inferSchema=True)\n",
    "\n",
    "#df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34aa9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e56a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3406f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
